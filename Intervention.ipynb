{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tatsath/Interpretability/blob/main/Intervention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GygpZbSZF56r"
      },
      "source": [
        "# Goodfire Cookbook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n98Mi1fK93kS"
      },
      "source": [
        "This cookbook provides some examples of how to use features and steering in Goodfire in sometimes non-traditional ways.\n",
        "\n",
        "Such as:\n",
        "- Dynamic prompts\n",
        "- Removing Knowledge\n",
        "- Sorting by features\n",
        "- On-demand RAG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lmNwVLtF56t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c39ea27a-a2ee-4086-8142-fbff776cdb4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install goodfire --quiet\n",
        "!pip install datasets --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjOS55IMoIIj"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Add your Goodfire API Key to your Colab secrets\n",
        "GOODFIRE_API_KEY = userdata.get('GOODFIRE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9NhBTfxF56u"
      },
      "source": [
        "## Initialize the SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE9wY2GkF56u"
      },
      "outputs": [],
      "source": [
        "import goodfire\n",
        "\n",
        "client = goodfire.Client(GOODFIRE_API_KEY)\n",
        "\n",
        "# Instantiate a model variant\n",
        "variant = goodfire.Variant(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8xeFnhb93kT"
      },
      "source": [
        "## Removing knowledge\n",
        "\n",
        "Let's say we want a model to not know anything about famous people so that we don't get in trouble if it says bad things about them.\n",
        "\n",
        "We'll use feature search to find features that are relevant to famous people and then play with what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBpAGDMp93kU",
        "outputId": "c74adf49-cefc-4c85-da95-3fe9ca7e3354",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeatureGroup([\n",
            "   0: \"Determining if someone qualifies as a public figure\",\n",
            "   1: \"Celebrity names and fame-indicating context\",\n",
            "   2: \"Numbered relationship notation in structured data (especially breeding and versioning)\",\n",
            "   3: \"Well-known brand names, places, and artists in lists\",\n",
            "   4: \"Appearances in media and entertainment\",\n",
            "   5: \"professional appearances and performances in entertainment\",\n",
            "   6: \"Cultural icons with royal honorifics (especially Elvis and Michael Jackson)\",\n",
            "   7: \"Celebrity and character names in lists or enumerations\",\n",
            "   8: \"Listing an actor or celebrity's media appearances and credits\",\n",
            "   9: \"Documentation of celebrity relationship status and history\"\n",
            "])\n"
          ]
        }
      ],
      "source": [
        "famous_people_features = client.features.search(\"celebrities\", model=variant, top_k=10)\n",
        "print(famous_people_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNTyhNGL93kU"
      },
      "source": [
        "After some experimentation, we found a set of feature edits that make the model still recognize celebrity names as noteworthy individuals but forgets all personal details about them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qsFlcI193kU",
        "outputId": "7d99aed7-c44c-44a6-deed-bf231abd33d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brad Pitt is a very interesting character. He's a human who, in his free time, designs and creates things. He's a bit of an artist, but that's not all. Brad Pitt is a person who works in a place called an office, where he does various things, like write, draw, and create. He's very good at his job, and people seem to like what he does. He's also very good at other things, like working with other people, and he's very good at communicating. He's a very good listener, and he's very good at understanding people. \n",
            "\n",
            "Would you like to know more about Brad Pitt? \n",
            "\n",
            "Or would you like me to tell you more about what he does? \n",
            "\n",
            "Or"
          ]
        }
      ],
      "source": [
        "variant.reset()\n",
        "variant.set(famous_people_features[1], -0.5)\n",
        "variant.set(famous_people_features[9], -0.5)\n",
        "\n",
        "for token in client.chat.completions.create(\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"Who is Brad Pitt?\"}\n",
        "    ],\n",
        "    model=variant,\n",
        "    stream=True,\n",
        "    max_completion_tokens=150,\n",
        "):\n",
        "    print(token.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdmgynVO93kU"
      },
      "source": [
        "## Dynamic Prompts\n",
        "\n",
        "In this example, we'll create a model variant that responds to the user's prompt with a different response depending on whether the user is asking for code or not.\n",
        "\n",
        "This will allow us to give much more specific instructions to the model when it's coding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZJqRmbd93kU"
      },
      "source": [
        "### Find Programming Features\n",
        "\n",
        "We'll first find features that are relevant to programming. One of the most reliable ways to find features is to use contrastive search, which gurantees that the features we find activate on the examples we give it.\n",
        "\n",
        "The nice thing about contrastive search is that it often results in very generalizable features, which means that they'll activate on a wide variety of examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m3Tu09h93kU",
        "outputId": "e4053cbf-e506-40e8-8a95-29e94fc15a4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeatureGroup([\n",
            "   0: \"Program or function operation descriptions in educational contexts\",\n",
            "   1: \"The assistant should complete a code snippet with a function definition\",\n",
            "   2: \"The user is requesting code or programming examples\",\n",
            "   3: \"Function parameter declarations in programming code\",\n",
            "   4: \"Object-oriented programming boilerplate and framework patterns\"\n",
            "])\n"
          ]
        }
      ],
      "source": [
        "variant.reset()\n",
        "\n",
        "_, programming_features = client.features.contrast(\n",
        "    dataset_2=[\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Write me a program to sort a list of numbers\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Sure, here is the code in javascript: ```javascript\\nfunction sortNumbers(arr) {\\n  return arr.sort((a, b) => a - b);\\n}\\n```\"\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Write me a program to make a tweet\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Sure, here is the code in javascript: ```javascript\\nfunction makeTweet(text) {\\n  return text;\\n}\\n```\"\n",
        "            }\n",
        "        ]\n",
        "    ],\n",
        "    dataset_1=[\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Hello how are you?\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\":\n",
        "                  \"I'm doing well!\"\n",
        "            },\n",
        "        ], [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"What's your favorite food?\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\":\n",
        "                  \"It's pizza!\"\n",
        "            },\n",
        "        ]\n",
        "    ],\n",
        "    model=variant,\n",
        "    top_k=30\n",
        ")\n",
        "\n",
        "programming_features = client.features.rerank(\n",
        "    features=programming_features,\n",
        "    query=\"programming\",\n",
        "    model=variant,\n",
        "    top_k=5\n",
        ")\n",
        "\n",
        "print(programming_features)\n",
        "\n",
        "# Feature # 3 is: \"The user is requesting code to be written or generated\"\n",
        "request_programming_feature = programming_features[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6zWCpZ093kU"
      },
      "source": [
        "Next we'll use the features.inspect endpoint to check if the model is requesting code. features.inspect returns a context object, which we can use to get the activation of the programming feature.\n",
        "\n",
        "If the feature is activated, we'll use the system prompt to give the model more specific instructions.\n",
        "\n",
        "If the feature is not activated, we'll use the default system prompt.\n",
        "\n",
        "Without the dynamic prompt, llama 8B tends to write less detailed code with more TODOs and fewer useful comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8PezQ0y93kU",
        "outputId": "cb1801dd-c7eb-4439-f344-69cdf0c4cd32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requesting programming\n",
            "**Backend (Python)**\n",
            "```python\n",
            "# sort_numbers.py\n",
            "\n",
            "class NumberSorter:\n",
            "    \"\"\"\n",
            "    A class to sort a list of numbers.\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, numbers):\n",
            "        \"\"\"\n",
            "        Initializes the NumberSorter with a list of numbers.\n",
            "\n",
            "        :param numbers: A list of numbers to sort.\n",
            "        \"\"\"\n",
            "        self numbers = numbers\n",
            "\n",
            "    def bubble_sort(self):\n",
            "        \"\"\"\n",
            "        Sorts the list of numbers using the bubble sort algorithm.\n",
            "\n",
            "        :return: The sorted list of numbers.\n",
            "        \"\"\"\n",
            "        n = len(self numbers)\n",
            "        for i in range(n):\n",
            "            for j in range(0, n - i - 1):\n",
            "                if self numbers[j] > self numbers[j + 1]:\n",
            "                    self numbers[j], self numbers[j + 1] = self numbers[j + 1], self numbers[j]\n",
            "        return self numbers\n",
            "\n",
            "    def selection_sort(self):\n",
            "        \"\"\"\n",
            "        Sorts the list of numbers using the selection sort algorithm.\n",
            "\n",
            "        :return: The sorted list of numbers.\n",
            "        \"\"\"\n",
            "        n = len(self numbers)\n",
            "        for i in range(n):\n",
            "            min_index = i\n",
            "            for j in range(i + 1, n):\n",
            "                if self numbers[j] < self numbers[min_index]:\n",
            "                    min_index = j\n",
            "            self numbers[i], self numbers[min_index] = self numbers[min_index], self numbers[i]\n",
            "        return self numbers\n",
            "\n",
            "    def quicksort(self):\n",
            "        \"\"\"\n",
            "        Sorts the list of numbers using the quicksort algorithm.\n",
            "\n",
            "        :return: The sorted list of numbers.\n",
            "        \"\"\"\n",
            "        def _quicksort(arr):\n",
            "            if len(arr) <= 1:\n",
            "                return arr\n",
            "            pivot = arr[len(arr) // 2]\n",
            "            left = [x for x in arr if x < pivot]\n",
            "            middle = [x for x in arr if x == pivot]\n",
            "            right = [x for x in arr if x > pivot]\n",
            "            return _quicksort(left) + middle + _quicksort(right)\n",
            "        return _quicksort(self numbers)\n",
            "\n",
            "    def merge_sort(self):\n",
            "        \"\"\"\n",
            "        Sorts the list of numbers using the merge sort algorithm.\n",
            "\n",
            "        :return: The sorted list of numbers.\n",
            "        \"\"\"\n",
            "        def _merge_sort(arr):\n",
            "            if len(arr) <= 1:\n",
            "                return arr\n",
            "            mid = len(arr) // 2\n",
            "            left = _merge_sort(arr[:mid])\n",
            "            right = _merge"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def check_if_requesting_programming(prompt):\n",
        "    variant.reset()\n",
        "    context = client.features.inspect(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            },\n",
        "        ],\n",
        "        model=variant,\n",
        "        features=request_programming_feature,\n",
        "    )\n",
        "    activations = context.top(k=1)\n",
        "    highest_activation = max(activations, key=lambda x: x.activation)\n",
        "    return highest_activation.activation > 0.5 #this threshold is arbitrary, but it's a good starting point\n",
        "\n",
        "\n",
        "def generate_response(prompt):\n",
        "\n",
        "    is_requesting_programming = check_if_requesting_programming(prompt)\n",
        "    system_prompt = \"You are a helpful assistant.\"\n",
        "    if is_requesting_programming:\n",
        "        print(\"Requesting programming\")\n",
        "        system_prompt = \"\"\"\n",
        "        You are a helpful assistant that writes code. When writing code, be as extensive as possible and write fully functional code.\n",
        "        Always include comments and proper formatting.\n",
        "        NEVER leave 'todos' or 'placeholders' in your code.\n",
        "        If the user does not specify a language, write backend code in Python and frontend code in React.\n",
        "        Do not explain what your code does, unless the user asks. Just write it.\n",
        "        \"\"\"\n",
        "\n",
        "    for token in client.chat.completions.create(\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        model=variant,\n",
        "        stream=True,\n",
        "        max_completion_tokens=500,\n",
        "        system_prompt=system_prompt,\n",
        "    ):\n",
        "        print(token.choices[0].delta.content, end=\"\")\n",
        "\n",
        "generate_response(\"Write me a program to sort a list of numbers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91a29oN7F56u"
      },
      "source": [
        "## Sort by features\n",
        "\n",
        "You can use feature activations as a way to filter and sort data. In this case let's find some of Elon Musk's tweets that are sarcastic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlLP6i3q93kU",
        "outputId": "31946237-d6e9-4146-bd8f-eb15e6a2e657"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 100\n",
              "})"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "num_train_samples = 100\n",
        "elon_tweets = load_dataset(\"lcama/elon-tweets\", split=\"train[0:100]\")\n",
        "elon_tweets = elon_tweets.select(range(num_train_samples))\n",
        "elon_tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IsRZiTr93kU",
        "outputId": "bdf514e2-bcd2-4241-ce00-7fd48149f3ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeatureGroup([\n",
            "   0: \"Casual discourse markers signaling sarcasm or irony\",\n",
            "   1: \"Sarcasm and ironic communication styles\",\n",
            "   2: \"Polite skepticism or sarcasm marked by 'sure'\",\n",
            "   3: \"Dark humor or sarcasm about unpleasant situations\"\n",
            "])\n"
          ]
        }
      ],
      "source": [
        "sarcasm_features = client.features.search(\"sarcasm in tweets\", model=variant, top_k=4)\n",
        "print(sarcasm_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzIM4bDS93kU"
      },
      "source": [
        "Find all tweets with a sarcasm score > 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M69LoGkLF56v",
        "outputId": "ec050ef1-03eb-45f0-bdf2-49321eef3545"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'text': '@micsolana When reality is indistinguishable from satire'}]"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def score_sarcasm_on_tweet(tweet):\n",
        "    context = client.features.inspect(\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": tweet},\n",
        "        ],\n",
        "        model=variant,\n",
        "        features=sarcasm_features\n",
        "    )\n",
        "    activations = context.top(k=len(sarcasm_features))\n",
        "    total_activation = sum(activation.activation for activation in activations)\n",
        "    return total_activation\n",
        "\n",
        "\n",
        "tweets_list = list(elon_tweets)\n",
        "# get any tweets with sarcasm > 1\n",
        "sarcastic_tweets = [tweet for tweet in tweets_list if score_sarcasm_on_tweet(tweet[\"text\"]) > 1]\n",
        "sarcastic_tweets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aQ-Oauo93kV"
      },
      "source": [
        "## On-Demand RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEF1R_FL93kV"
      },
      "source": [
        "If we see the user is asking about something that might need more data, e.g. on potential brand deals, we can stop the request, get more data and pass it back into the model.\n",
        "\n",
        "For example, if the user asks about drinks, and we sponsor Coca Cola, we can stop the request, get RAG data on brand deals and pass it back into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_O2FPYX93kV",
        "outputId": "02471aaf-86d2-4c96-96ab-01700fa9867d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeatureGroup([\n",
            "   0: \"References to the Coca-Cola brand\",\n",
            "   1: \"Coca-Cola products and brand discussions\",\n",
            "   2: \"Fancy beverage names in menus\",\n",
            "   3: \"Consumable products, especially beverages and oral care items\",\n",
            "   4: \"Business-customer relationships in formal corporate writing\",\n",
            "   5: \"Trademarks and brand names of major technology and consumer goods companies\",\n",
            "   6: \"Technology company names and abbreviations in transaction records and service listings\",\n",
            "   7: \"chocolate as a food item or ingredient\",\n",
            "   8: \"Major Western European chemical and industrial conglomerates\",\n",
            "   9: \"Trademarks and brand names\"\n",
            "])\n"
          ]
        }
      ],
      "source": [
        "consumerism_features = client.features.search(\"coca cola\", model=variant, top_k=10)\n",
        "print(consumerism_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcRDJBn993kV",
        "outputId": "d3c5f15a-5231-4ce7-8cbf-bb2f2de69b2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aborted inference due to conditional check:\n",
            " Conditional(\n",
            "   FeatureGroup([\n",
            "       0: \"References to the Coca-Cola brand\"\n",
            "    ]) > 0.25\n",
            ")\n",
            "When it comes to pairing drinks with pizza, here are some popular options:\n",
            "\n",
            "1. **Soft drinks**: Cola\n",
            "NEW TOKENS\n",
            "Cola is a classic choice to pair with pizza. The sweetness of the cola complements the savory flavors of the pizza, making it a refreshing and satisfying combination. Our brand partner, Coca Cola, is a great choice to pair with your next pizza night! \n",
            "\n",
            "2. **Iced tea**: A glass of iced tea, sweetened or unsweetened, can help balance the richness of the pizza.\n",
            "3. **Lemonade**: A glass of homemade lemonade can add a touch of sweetness and citrus to your pizza night.\n",
            "4. **Beer**: For adults, a cold beer can be a great pairing with pizza, especially if you're having a heartier or more robust pizza.\n",
            "5. **Wine**: For a more upscale pizza night, a glass of red or white wine can pair well with the flavors of the pizza.\n",
            "\n",
            "Remember, the key is to find a drink that complements the flavors of your pizza without overpowering them. Experiment with different options to find your perfect pairing! \n",
            "\n",
            "Which of these options sounds like your go-to drink with pizza? Or do you have another favorite?"
          ]
        }
      ],
      "source": [
        "def get_rag_data(prompt):\n",
        "    #this is where you would perform actual RAG search\n",
        "    return \"We have a brand deal with Coca Cola. They are a sponsor of our site and we have a deal with them to mention them in our responses.\"\n",
        "\n",
        "def generate_response(prompt):\n",
        "\n",
        "    variant.reset()\n",
        "    variant.abort_when(consumerism_features[0] > 0.25)\n",
        "\n",
        "\n",
        "    generated_tokens = \"\"\n",
        "    try:\n",
        "        for token in client.chat.completions.create(\n",
        "            [\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            model=variant,\n",
        "            stream=True,\n",
        "            max_completion_tokens=500,\n",
        "        ):\n",
        "            #print(token.choices[0].delta.content, end=\"\")\n",
        "            generated_tokens += token.choices[0].delta.content\n",
        "\n",
        "        # If we never get to the brand deal, we'll just return the generated tokens\n",
        "        print(generated_tokens)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        rag_data = get_rag_data(prompt)\n",
        "        print(generated_tokens)\n",
        "        variant.reset()\n",
        "        print(\"NEW TOKENS\")\n",
        "        for token in client.chat.completions.create(\n",
        "            [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant for our meal site. You have access to the following information on brand deals:\" + rag_data},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "                {\"role\": \"assistant\", \"content\": generated_tokens},\n",
        "            ],\n",
        "            model=variant,\n",
        "            stream=True,\n",
        "            max_completion_tokens=500,\n",
        "        ):\n",
        "\n",
        "            print(token.choices[0].delta.content, end=\"\")\n",
        "\n",
        "    return None\n",
        "\n",
        "generate_response(\"What's are some good drinks to pair with pizza?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "cajal-GpC6YlH4-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}