{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tatsath/Interpretability/blob/main/Intervention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n98Mi1fK93kS"
      },
      "source": [
        "This cookbook provides some examples of how to use features and steering in Goodfire in sometimes non-traditional ways.\n",
        "\n",
        "Such as:\n",
        "- Dynamic prompts\n",
        "- Removing Knowledge\n",
        "- Sorting by features\n",
        "- On-demand RAG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lmNwVLtF56t",
        "outputId": "03430c89-40e6-4475-be6b-0b95515c3939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m970.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install goodfire --quiet\n",
        "!pip install datasets --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjOS55IMoIIj"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Add your Goodfire API Key to your Colab secrets\n",
        "GOODFIRE_API_KEY = userdata.get('GOODFIRE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9NhBTfxF56u"
      },
      "source": [
        "## Initialize the SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE9wY2GkF56u"
      },
      "outputs": [],
      "source": [
        "import goodfire\n",
        "\n",
        "client = goodfire.Client(GOODFIRE_API_KEY)\n",
        "\n",
        "# Instantiate a model variant\n",
        "variant = goodfire.Variant(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8xeFnhb93kT"
      },
      "source": [
        "## Removing knowledge\n",
        "\n",
        "Let's say we want a model to not know anything about famous people so that we don't get in trouble if it says bad things about them.\n",
        "\n",
        "We'll use feature search to find features that are relevant to famous people and then play with what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBpAGDMp93kU"
      },
      "outputs": [],
      "source": [
        "famous_people_features = client.features.search(\"celebrities\", model=variant, top_k=10)\n",
        "print(famous_people_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNTyhNGL93kU"
      },
      "source": [
        "After some experimentation, we found a set of feature edits that make the model still recognize celebrity names as noteworthy individuals but forgets all personal details about them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qsFlcI193kU"
      },
      "outputs": [],
      "source": [
        "variant.reset()\n",
        "variant.set(famous_people_features[1], -0.5)\n",
        "variant.set(famous_people_features[9], -0.5)\n",
        "\n",
        "for token in client.chat.completions.create(\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"Who is Brad Pitt?\"}\n",
        "    ],\n",
        "    model=variant,\n",
        "    stream=True,\n",
        "    max_completion_tokens=150,\n",
        "):\n",
        "    print(token.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdmgynVO93kU"
      },
      "source": [
        "## Dynamic Prompts\n",
        "\n",
        "In this example, we'll create a model variant that responds to the user's prompt with a different response depending on whether the user is asking for code or not.\n",
        "\n",
        "This will allow us to give much more specific instructions to the model when it's coding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZJqRmbd93kU"
      },
      "source": [
        "### Find Programming Features\n",
        "\n",
        "We'll first find features that are relevant to programming. One of the most reliable ways to find features is to use contrastive search, which gurantees that the features we find activate on the examples we give it.\n",
        "\n",
        "The nice thing about contrastive search is that it often results in very generalizable features, which means that they'll activate on a wide variety of examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m3Tu09h93kU"
      },
      "outputs": [],
      "source": [
        "variant.reset()\n",
        "\n",
        "_, programming_features = client.features.contrast(\n",
        "    dataset_2=[\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Write me a program to sort a list of numbers\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Sure, here is the code in javascript: ```javascript\\nfunction sortNumbers(arr) {\\n  return arr.sort((a, b) => a - b);\\n}\\n```\"\n",
        "            }\n",
        "        ],\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Write me a program to make a tweet\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"Sure, here is the code in javascript: ```javascript\\nfunction makeTweet(text) {\\n  return text;\\n}\\n```\"\n",
        "            }\n",
        "        ]\n",
        "    ],\n",
        "    dataset_1=[\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Hello how are you?\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\":\n",
        "                  \"I'm doing well!\"\n",
        "            },\n",
        "        ], [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"What's your favorite food?\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\":\n",
        "                  \"It's pizza!\"\n",
        "            },\n",
        "        ]\n",
        "    ],\n",
        "    model=variant,\n",
        "    top_k=30\n",
        ")\n",
        "\n",
        "programming_features = client.features.rerank(\n",
        "    features=programming_features,\n",
        "    query=\"programming\",\n",
        "    model=variant,\n",
        "    top_k=5\n",
        ")\n",
        "\n",
        "print(programming_features)\n",
        "\n",
        "# Feature # 3 is: \"The user is requesting code to be written or generated\"\n",
        "request_programming_feature = programming_features[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6zWCpZ093kU"
      },
      "source": [
        "Next we'll use the features.inspect endpoint to check if the model is requesting code. features.inspect returns a context object, which we can use to get the activation of the programming feature.\n",
        "\n",
        "If the feature is activated, we'll use the system prompt to give the model more specific instructions.\n",
        "\n",
        "If the feature is not activated, we'll use the default system prompt.\n",
        "\n",
        "Without the dynamic prompt, llama 8B tends to write less detailed code with more TODOs and fewer useful comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8PezQ0y93kU"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def check_if_requesting_programming(prompt):\n",
        "    variant.reset()\n",
        "    context = client.features.inspect(\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            },\n",
        "        ],\n",
        "        model=variant,\n",
        "        features=request_programming_feature,\n",
        "    )\n",
        "    activations = context.top(k=1)\n",
        "    highest_activation = max(activations, key=lambda x: x.activation)\n",
        "    return highest_activation.activation > 0.5 #this threshold is arbitrary, but it's a good starting point\n",
        "\n",
        "\n",
        "def generate_response(prompt):\n",
        "\n",
        "    is_requesting_programming = check_if_requesting_programming(prompt)\n",
        "    system_prompt = \"You are a helpful assistant.\"\n",
        "    if is_requesting_programming:\n",
        "        print(\"Requesting programming\")\n",
        "        system_prompt = \"\"\"\n",
        "        You are a helpful assistant that writes code. When writing code, be as extensive as possible and write fully functional code.\n",
        "        Always include comments and proper formatting.\n",
        "        NEVER leave 'todos' or 'placeholders' in your code.\n",
        "        If the user does not specify a language, write backend code in Python and frontend code in React.\n",
        "        Do not explain what your code does, unless the user asks. Just write it.\n",
        "        \"\"\"\n",
        "\n",
        "    for token in client.chat.completions.create(\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        model=variant,\n",
        "        stream=True,\n",
        "        max_completion_tokens=500,\n",
        "        system_prompt=system_prompt,\n",
        "    ):\n",
        "        print(token.choices[0].delta.content, end=\"\")\n",
        "\n",
        "generate_response(\"Write me a program to sort a list of numbers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91a29oN7F56u"
      },
      "source": [
        "## Sort by features\n",
        "\n",
        "You can use feature activations as a way to filter and sort data. In this case let's find some of Elon Musk's tweets that are sarcastic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlLP6i3q93kU"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "num_train_samples = 100\n",
        "elon_tweets = load_dataset(\"lcama/elon-tweets\", split=\"train[0:100]\")\n",
        "elon_tweets = elon_tweets.select(range(num_train_samples))\n",
        "elon_tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IsRZiTr93kU"
      },
      "outputs": [],
      "source": [
        "sarcasm_features = client.features.search(\"sarcasm in tweets\", model=variant, top_k=4)\n",
        "print(sarcasm_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzIM4bDS93kU"
      },
      "source": [
        "Find all tweets with a sarcasm score > 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M69LoGkLF56v"
      },
      "outputs": [],
      "source": [
        "def score_sarcasm_on_tweet(tweet):\n",
        "    context = client.features.inspect(\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": tweet},\n",
        "        ],\n",
        "        model=variant,\n",
        "        features=sarcasm_features\n",
        "    )\n",
        "    activations = context.top(k=len(sarcasm_features))\n",
        "    total_activation = sum(activation.activation for activation in activations)\n",
        "    return total_activation\n",
        "\n",
        "\n",
        "tweets_list = list(elon_tweets)\n",
        "# get any tweets with sarcasm > 1\n",
        "sarcastic_tweets = [tweet for tweet in tweets_list if score_sarcasm_on_tweet(tweet[\"text\"]) > 1]\n",
        "sarcastic_tweets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aQ-Oauo93kV"
      },
      "source": [
        "## On-Demand RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEF1R_FL93kV"
      },
      "source": [
        "If we see the user is asking about something that might need more data, e.g. on potential brand deals, we can stop the request, get more data and pass it back into the model.\n",
        "\n",
        "For example, if the user asks about drinks, and we sponsor Coca Cola, we can stop the request, get RAG data on brand deals and pass it back into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_O2FPYX93kV"
      },
      "outputs": [],
      "source": [
        "consumerism_features = client.features.search(\"coca cola\", model=variant, top_k=10)\n",
        "print(consumerism_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcRDJBn993kV"
      },
      "outputs": [],
      "source": [
        "def get_rag_data(prompt):\n",
        "    #this is where you would perform actual RAG search\n",
        "    return \"We have a brand deal with Coca Cola. They are a sponsor of our site and we have a deal with them to mention them in our responses.\"\n",
        "\n",
        "def generate_response(prompt):\n",
        "\n",
        "    variant.reset()\n",
        "    variant.abort_when(consumerism_features[0] > 0.25)\n",
        "\n",
        "\n",
        "    generated_tokens = \"\"\n",
        "    try:\n",
        "        for token in client.chat.completions.create(\n",
        "            [\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            model=variant,\n",
        "            stream=True,\n",
        "            max_completion_tokens=500,\n",
        "        ):\n",
        "            #print(token.choices[0].delta.content, end=\"\")\n",
        "            generated_tokens += token.choices[0].delta.content\n",
        "\n",
        "        # If we never get to the brand deal, we'll just return the generated tokens\n",
        "        print(generated_tokens)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        rag_data = get_rag_data(prompt)\n",
        "        print(generated_tokens)\n",
        "        variant.reset()\n",
        "        print(\"NEW TOKENS\")\n",
        "        for token in client.chat.completions.create(\n",
        "            [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant for our meal site. You have access to the following information on brand deals:\" + rag_data},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "                {\"role\": \"assistant\", \"content\": generated_tokens},\n",
        "            ],\n",
        "            model=variant,\n",
        "            stream=True,\n",
        "            max_completion_tokens=500,\n",
        "        ):\n",
        "\n",
        "            print(token.choices[0].delta.content, end=\"\")\n",
        "\n",
        "    return None\n",
        "\n",
        "generate_response(\"What's are some good drinks to pair with pizza?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "9704210eed1ca24cd2f085f9e4210fc50908199b88d7eec1964ef766a717df17"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
