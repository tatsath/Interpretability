{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neaCBPHeEz9L"
      },
      "source": [
        "# Attribution Patching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TVAuWSgqKK0"
      },
      "source": [
        "📗 This tutorial is adapted from Neel Nanda’s [blog post](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching).\n",
        "\n",
        "Activation patching is a method to determine how model components influence model computations (see our activation patching tutorial for more information). Although activation patching is a useful tool for circuit identification, it requires a separate forward pass through the model for each patched activation, making it time- and resource-intensive.\n",
        "\n",
        "**Attribution patching** uses gradients to take a linear approximation to activation patching and can be done simultaneously in two forward and one backward pass, making it much more scalable to large models.\n",
        "\n",
        "You can find a colab version of the tutorial [here](https://colab.research.google.com/github/ndif-team/nnsight/blob/main/docs/source/notebooks/tutorials/attribution_patching.ipynb) or Neel’s version [here](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Attribution_Patching_Demo.ipynb).\n",
        "\n",
        "Read more about an application of Attribution Patching in [Attribution Patching Outperforms Automated Circuit Discovery](https://arxiv.org/abs/2310.10348). 📙"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u0VwEtiOJLZ"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZplUJFI61JX"
      },
      "source": [
        "If you are using Colab or haven't yet installed NNsight, install the package:\n",
        "```\n",
        "!pip install -U nnsight\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "-Z0oxBkGrFkF",
        "outputId": "6cf0df0e-8858-4a97-92af-e638692d5136",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nnsight in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from nnsight) (4.48.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from nnsight) (4.25.6)\n",
            "Requirement already satisfied: python-socketio[client] in /usr/local/lib/python3.11/dist-packages (from nnsight) (5.12.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from nnsight) (0.21.0)\n",
            "Requirement already satisfied: pydantic>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from nnsight) (2.10.6)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from nnsight) (2.5.1+cu124)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from nnsight) (0.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from nnsight) (0.20.1+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from nnsight) (1.3.0)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from nnsight) (0.32.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from nnsight) (0.8.1)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from nnsight) (0.19.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from nnsight) (0.10.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from nnsight) (7.34.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.0->nnsight) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.0->nnsight) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.0->nnsight) (4.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.0->nnsight) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->nnsight) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->nnsight) (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate->nnsight) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate->nnsight) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->nnsight) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate->nnsight) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate->nnsight) (0.5.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->nnsight) (8.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers->nnsight) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers->nnsight) (2.32.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers->nnsight) (11.1.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->nnsight) (4.9.0)\n",
            "Requirement already satisfied: bidict>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from python-socketio[client]->nnsight) (0.23.1)\n",
            "Requirement already satisfied: python-engineio>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from python-socketio[client]->nnsight) (4.11.2)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from python-socketio[client]->nnsight) (1.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->nnsight) (4.67.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->nnsight) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->nnsight) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->nnsight) (0.2.13)\n",
            "Requirement already satisfied: simple-websocket>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from python-engineio>=4.11.0->python-socketio[client]->nnsight) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers->nnsight) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers->nnsight) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers->nnsight) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers->nnsight) (2025.1.31)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->nnsight) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->nnsight) (3.0.2)\n",
            "Requirement already satisfied: wsproto in /usr/local/lib/python3.11/dist-packages (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio[client]->nnsight) (1.2.0)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto->simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio[client]->nnsight) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    is_colab = True\n",
        "except ImportError:\n",
        "    is_colab = False\n",
        "\n",
        "if is_colab:\n",
        "    !pip install -U nnsight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s12FEmhs61bp"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "qM0t9saED43y"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import einops\n",
        "import torch\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\" if is_colab else \"plotly_mimetype+notebook_connected+notebook\"\n",
        "\n",
        "\n",
        "from nnsight import LanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "36SE-gB4rFkH",
        "outputId": "789824a3-da0d-4414-a0ed-c4093d34785e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4.3\n"
          ]
        }
      ],
      "source": [
        "import nnsight\n",
        "print(nnsight.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xBRXG4iOQEA"
      },
      "source": [
        "## 1️⃣ Indirect Object Identification (IOI) Patching\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwPLS9lHqH1F"
      },
      "source": [
        "Indirect object identification (IOI) is the ability to infer the correct indirect object in a sentence, allowing one to complete sentences like \"John and Mary went to the shops, John gave a bag to\" with the correct answer \" Mary\". Understanding how language models like GPT-2 perform linguistic tasks like IOI helps us gain insights into their internal mechanisms and decision-making processes.\n",
        "\n",
        "Here, we apply the [IOI task](https://arxiv.org/abs/2211.00593) to explore how GPT-2 small is performing IOI with attribution patching.\n",
        "\n",
        "*📚 Note: For more detail on the IOI task, check out the [ARENA walkthrough](https://arena3-chapter1-transformer-interp.streamlit.app/[1.4.1]_Indirect_Object_Identification).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJQMmrRDOIZ5",
        "outputId": "e7d14311-bd9b-4713-9300-c6af416bdded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "  (generator): Generator(\n",
            "    (streamer): Streamer()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = LanguageModel(\"openai-community/gpt2\", device_map=\"auto\", dispatch=True)\n",
        "clear_output()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6SqGyDcct1H"
      },
      "source": [
        "Looking at the model architecture, we can see there are 12 layers, each with 12 GPT-2 Blocks. We will use attribution patching to approximate the contribution of each layer and each attention head for the IOI task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRTEP_c2_c2L"
      },
      "source": [
        "We next define 8 IOI prompts, with each prompt having one related corrupted prompt variation (i.e., the indirect object is swapped out)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "bcyK0AoeObY_"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"When John and Mary went to the shops, John gave the bag to\",\n",
        "    \"When John and Mary went to the shops, Mary gave the bag to\",\n",
        "    \"When Tom and James went to the park, James gave the ball to\",\n",
        "    \"When Tom and James went to the park, Tom gave the ball to\",\n",
        "    \"When Dan and Sid went to the shops, Sid gave an apple to\",\n",
        "    \"When Dan and Sid went to the shops, Dan gave an apple to\",\n",
        "    \"After Martin and Amy went to the park, Amy gave a drink to\",\n",
        "    \"After Martin and Amy went to the park, Martin gave a drink to\",\n",
        "]\n",
        "\n",
        "# Answers are each formatted as (correct, incorrect):\n",
        "answers = [\n",
        "    (\" Mary\", \" John\"),\n",
        "    (\" John\", \" Mary\"),\n",
        "    (\" Tom\", \" James\"),\n",
        "    (\" James\", \" Tom\"),\n",
        "    (\" Dan\", \" Sid\"),\n",
        "    (\" Sid\", \" Dan\"),\n",
        "    (\" Martin\", \" Amy\"),\n",
        "    (\" Amy\", \" Martin\"),\n",
        "]\n",
        "\n",
        "# Tokenize clean and corrupted inputs:\n",
        "clean_tokens = model.tokenizer(prompts, return_tensors=\"pt\")[\"input_ids\"]\n",
        "# The associated corrupted input is the prompt after the current clean prompt\n",
        "# for even indices, or the prompt prior to the current clean prompt for odd indices\n",
        "corrupted_tokens = clean_tokens[\n",
        "    [(i + 1 if i % 2 == 0 else i - 1) for i in range(len(clean_tokens))]\n",
        "]\n",
        "\n",
        "# Tokenize answers for each prompt:\n",
        "answer_token_indices = torch.tensor(\n",
        "    [\n",
        "        [model.tokenizer(answers[i][j])[\"input_ids\"][0] for j in range(2)]\n",
        "        for i in range(len(answers))\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"If a company's revenue grows and expenses stay flat, profit will likely\",\n",
        "    \"If a company's revenue shrinks and expenses stay flat, profit will likely\",\n",
        "    \"When inflation is high and rates are low, real returns will likely\",\n",
        "    \"When inflation is low and rates are high, real returns will likely\",\n",
        "    \"If a firm's debt is high and cash is low, its risk will likely\",\n",
        "    \"If a firm's debt is low and cash is high, its risk will likely\",\n",
        "    \"After a firm cuts costs and raises prices, margins will likely\",\n",
        "    \"After a firm raises costs and cuts prices, margins will likely\",\n",
        "]\n",
        "\n",
        "# Answers are each formatted as (correct, incorrect):\n",
        "answers = [\n",
        "    (\"increase\", \"decrease\"),\n",
        "    (\"decrease\", \"increase\"),\n",
        "    (\"decrease\", \"increase\"),\n",
        "    (\"increase\", \"decrease\"),\n",
        "    (\"increase\", \"decrease\"),\n",
        "    (\"decrease\", \"increase\"),\n",
        "    (\"increase\", \"decrease\"),\n",
        "    (\"decrease\", \"increase\"),\n",
        "]\n",
        "\n",
        "# Tokenize clean and corrupted inputs:\n",
        "clean_tokens = model.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
        "# The associated corrupted input is the prompt after the current clean prompt\n",
        "# for even indices, or the prompt prior to the current clean prompt for odd indices\n",
        "corrupted_tokens = clean_tokens[\n",
        "    [(i + 1 if i % 2 == 0 else i - 1) for i in range(len(clean_tokens))]\n",
        "]\n",
        "\n",
        "# Tokenize answers for each prompt:\n",
        "answer_token_indices = torch.tensor(\n",
        "    [\n",
        "        [model.tokenizer(answers[i][j])[\"input_ids\"][0] for j in range(2)]\n",
        "        for i in range(len(answers))\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "UE_dLnnp8urb"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"If a company's revenue grows steadily, its stock price will likely\", # Tests understanding of revenue growth and stock market response\n",
        "    \"A company with high debt and low cash flow is considered financially\", # Tests understanding of financial risk and stability\n",
        "    \"During periods of economic recession, consumer spending typically\", # Tests understanding of macroeconomic factors and consumer behavior\n",
        "    \"When interest rates rise, bond prices generally tend to\", # Tests understanding of interest rate and bond market dynamics\n",
        "    \"A company with a diversified portfolio of investments is less\", # Tests understanding of investment diversification and risk reduction\n",
        "    \"If a company consistently reports strong earnings, its investors are likely to be\", # Tests understanding of financial performance and investor sentiment\n",
        "    \"When inflation increases, the purchasing power of money\", # Tests understanding of inflation and its impact on currency value\n",
        "    \"A company with a strong brand reputation often commands a higher\", # Tests understanding of intangible assets and brand value\n",
        "]\n",
        "\n",
        "# Answers are each formatted as (correct, incorrect):\n",
        "answers = [\n",
        "    (\"increase\", \"decrease\"),\n",
        "    (\"risky\", \"stable\"),\n",
        "    (\"decrease\", \"increase\"),\n",
        "    (\"fall\", \"rise\"),\n",
        "    (\"risky\", \"safe\"),\n",
        "    (\"satisfied\", \"dissatisfied\"),\n",
        "    (\"decreases\", \"increases\"),\n",
        "    (\"price\", \"discount\"),\n",
        "]\n",
        "\n",
        "# Tokenize clean and corrupted inputs:\n",
        "clean_tokens = model.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"]\n",
        "# The associated corrupted input is the prompt after the current clean prompt\n",
        "# for even indices, or the prompt prior to the current clean prompt for odd indices\n",
        "corrupted_tokens = clean_tokens[\n",
        "    [(i + 1 if i % 2 == 0 else i - 1) for i in range(len(clean_tokens))]\n",
        "]\n",
        "\n",
        "# Tokenize answers for each prompt:\n",
        "answer_token_indices = torch.tensor(\n",
        "    [\n",
        "        [model.tokenizer(answers[i][j])[\"input_ids\"][0] for j in range(2)]\n",
        "        for i in range(len(answers))\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "agBMMUPJH2Mg"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Meaning for Attribution Patching\n",
        "\n",
        "\"If a company's revenue grows steadily, its stock price will likely...\"\n",
        "\n",
        "Attribution Patching Focus: This prompt aims to identify attention heads that are sensitive to the relationship between revenue growth and stock market performance. Patching these heads could reveal how the model connects fundamental financial indicators to market expectations.\n",
        "\"A company with high debt and low cash flow is considered financially...\"\n",
        "\n",
        "Attribution Patching Focus: This prompt targets attention heads that assess financial risk and stability. Patching these heads could shed light on how the model evaluates the financial health of a company based on its debt and cash flow situation.\n",
        "\"During periods of economic recession, consumer spending typically...\"\n",
        "\n",
        "Attribution Patching Focus: This prompt focuses on attention heads that capture macroeconomic influences on consumer behavior. Patching these heads could reveal how the model understands the relationship between economic conditions and consumer spending patterns.\n",
        "\"When interest rates rise, bond prices generally tend to...\"\n",
        "\n",
        "Attribution Patching Focus: This prompt aims to identify attention heads that understand the inverse relationship between interest rates and bond prices. Patching these heads could provide insights into how the model processes financial market dynamics.\n",
        "\"A company with a diversified portfolio of investments is less...\"\n",
        "\n",
        "Attribution Patching Focus: This prompt targets attention heads that recognize the benefits of investment diversification. Patching these heads could reveal how the model evaluates risk reduction strategies in investment portfolios.\n",
        "\"If a company consistently reports strong earnings, its investors are likely to be...\"\n",
        "\n",
        "Attribution Patching Focus: This prompt focuses on attention heads that connect financial performance with investor sentiment. Patching these heads could provide insights into how the model understands the relationship between a company's earnings and investor satisfaction.\n",
        "\"When inflation increases, the purchasing power of money...\"\n",
        "\n",
        "Attribution Patching Focus: This prompt aims to identify attention heads that understand the impact of inflation on currency value. Patching these heads could reveal how the model processes macroeconomic factors and their effects on purchasing power.\n",
        "\"A company with a strong brand reputation often commands a higher...\"\n",
        "\n",
        "Attribution Patching Focus: This prompt targets attention heads that recognize the value of intangible assets like brand reputation. Patching these heads could shed light on how the model evaluates brand strength and its influence on pricing or market position.\n",
        "Overall, by using these distinct financial prompts for attribution patching, we can gain a more nuanced understanding of which attention heads are responsible for specific aspects of financial reasoning within the language model. This information can be used to improve the model's performance, explainability, and its ability to handle diverse financial scenarios."
      ],
      "metadata": {
        "id": "eREZ_79pH7Un"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGDjpcgZOg6u"
      },
      "source": [
        "Next, we create a function to calculate the mean logit difference for the correct vs incorrect answer tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "05T3hf3LOpFG"
      },
      "outputs": [],
      "source": [
        "def get_logit_diff(logits, answer_token_indices=answer_token_indices):\n",
        "    logits = logits[:, -1, :]\n",
        "    correct_logits = logits.gather(1, answer_token_indices[:, 0].unsqueeze(1))\n",
        "    incorrect_logits = logits.gather(1, answer_token_indices[:, 1].unsqueeze(1))\n",
        "    return (correct_logits - incorrect_logits).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQWBAbKeAwy9"
      },
      "source": [
        "We then calculate the logit difference for both the clean and the corrupted baselines.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X647xzaUAvwE",
        "outputId": "74c235a2-5815-4c72-e3ff-a29c67f472aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean logit diff: 0.8367\n",
            "Corrupted logit diff: 0.2636\n"
          ]
        }
      ],
      "source": [
        "clean_logits = model.trace(clean_tokens, trace=False).logits.cpu()\n",
        "corrupted_logits = model.trace(corrupted_tokens, trace=False).logits.cpu()\n",
        "\n",
        "CLEAN_BASELINE = get_logit_diff(clean_logits, answer_token_indices).item()\n",
        "print(f\"Clean logit diff: {CLEAN_BASELINE:.4f}\")\n",
        "\n",
        "CORRUPTED_BASELINE = get_logit_diff(corrupted_logits, answer_token_indices).item()\n",
        "print(f\"Corrupted logit diff: {CORRUPTED_BASELINE:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EpGxahUO8qZ"
      },
      "source": [
        "Now let's define an `ioi_metric` function to evaluate patched IOI changes normalized to our clean and corruped baselines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jUUNxJcPDNJ",
        "outputId": "cb01a7e2-cb69-4052-f479-bd4bfaff6ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean Baseline is 1: 1.0000\n",
            "Corrupted Baseline is 0: 0.0000\n"
          ]
        }
      ],
      "source": [
        "def ioi_metric(\n",
        "    logits,\n",
        "    answer_token_indices=answer_token_indices,\n",
        "):\n",
        "    return (get_logit_diff(logits, answer_token_indices) - CORRUPTED_BASELINE) / (\n",
        "        CLEAN_BASELINE - CORRUPTED_BASELINE\n",
        "    )\n",
        "\n",
        "print(f\"Clean Baseline is 1: {ioi_metric(clean_logits).item():.4f}\")\n",
        "print(f\"Corrupted Baseline is 0: {ioi_metric(corrupted_logits).item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNHif8pqPD6e"
      },
      "source": [
        "## 2️⃣ Attribution Patching Over Components\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8-cPY-YqFId"
      },
      "source": [
        "Attribution patching is a technique that uses gradients to take a linear approximation to activation patching. The key assumption is that the corrupted run is a locally linear function of its activations.\n",
        "\n",
        "We thus take the gradient of the patch metric (`ioi_metric`) with respect to its activations, where we consider a patch of activations to be applying `corrupted_x` to `corrupted_x + (clean_x - corrupted_x)`. Then, we compute the patch metric's change: `(corrupted_grad_x * (clean_x - corrupted_x)).sum()`. All we need to do is take a backwards pass on the corrupted prompt with respect to the patch metric and cache all gradients with respect to the activations.\n",
        "\n",
        "Let’s see how this breaks down in NNsight!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guI294-EPSMG"
      },
      "source": [
        "**A note on c_proj:** *Most HuggingFace models don’t have nice individual attention head representations to hook. Instead, we have the linear layer `c_proj` which implicitly combines the “projection per attention head” and the “sum over attention head” operations. See [this snippet](https://arena3-chapter1-transformer-interp.streamlit.app/~/+/[1.4.2]_Function_Vectors_&_Model_Steering) from ARENA for more information.*\n",
        "\n",
        "TL;DR: We will use the input to `c_proj` for causal interventions on a particular attention head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "QdSrOsXvsjjb"
      },
      "outputs": [],
      "source": [
        "clean_out = []\n",
        "corrupted_out = []\n",
        "corrupted_grads = []\n",
        "\n",
        "with model.trace() as tracer:\n",
        "# Using nnsight's tracer.invoke context, we can batch the clean and the\n",
        "# corrupted runs into the same tracing context, allowing us to access\n",
        "# information generated within each of these runs within one forward pass\n",
        "\n",
        "    with tracer.invoke(clean_tokens) as invoker_clean:\n",
        "        # Gather each layer's attention\n",
        "        for layer in model.transformer.h:\n",
        "            # Get clean attention output for this layer\n",
        "            # across all attention heads\n",
        "            attn_out = layer.attn.c_proj.input\n",
        "            clean_out.append(attn_out.save())\n",
        "\n",
        "    with tracer.invoke(corrupted_tokens) as invoker_corrupted:\n",
        "        # Gather each layer's attention and gradients\n",
        "        for layer in model.transformer.h:\n",
        "            # Get corrupted attention output for this layer\n",
        "            # across all attention heads\n",
        "            attn_out = layer.attn.c_proj.input\n",
        "            corrupted_out.append(attn_out.save())\n",
        "            # save corrupted gradients for attribution patching\n",
        "            corrupted_grads.append(attn_out.grad.save())\n",
        "\n",
        "        # Let's get the logits for the model's output\n",
        "        # for the corrupted run\n",
        "        logits = model.lm_head.output.save()\n",
        "\n",
        "        # Our metric uses tensors saved on cpu, so we\n",
        "        # need to move the logits to cpu.\n",
        "        value = ioi_metric(logits.cpu())\n",
        "\n",
        "        # We also need to run a backwards pass to\n",
        "        # update gradient values\n",
        "        value.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7AsfrICskxH"
      },
      "source": [
        "Next, for a given activation we compute `(corrupted_grad_act * (clean_act - corrupted_act)).sum()`. We use `einops.reduce` to rearrange and sum activations over the correct dimension. In this case, we want to estimate the effect of specific attention heads, so we sum over heads rather than token position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "cxC8Xs8HsqWm"
      },
      "outputs": [],
      "source": [
        "patching_results = []\n",
        "\n",
        "for corrupted_grad, corrupted, clean, layer in zip(\n",
        "    corrupted_grads, corrupted_out, clean_out, range(len(clean_out))\n",
        "):\n",
        "\n",
        "    residual_attr = einops.reduce(\n",
        "        corrupted_grad.value[:,-1,:] * (clean.value[:,-1,:] - corrupted.value[:,-1,:]),\n",
        "        \"batch (head dim) -> head\",\n",
        "        \"sum\",\n",
        "        head = 12,\n",
        "        dim = 64,\n",
        "    )\n",
        "\n",
        "    patching_results.append(\n",
        "        residual_attr.detach().cpu().numpy()\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "AdRvJdKtsrBv",
        "outputId": "5af38a1a-1145-46e1-b6aa-61a3c0e78dfd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1822e1c9-ae24-4173-b390-345980bf73e9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1822e1c9-ae24-4173-b390-345980bf73e9\")) {                    Plotly.newPlot(                        \"1822e1c9-ae24-4173-b390-345980bf73e9\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[-0.045902453,0.0228047,0.018858535,0.040641323,-0.046602845,0.029526481,-0.007725872,0.029326448,0.029389508,0.033750728,0.029176813,-0.0026753072],[-0.045575574,-0.019046817,0.0026423836,-0.0017230074,0.0060490696,0.02218195,0.0020110696,-0.04597455,-0.005481582,-0.0020163525,-0.015831701,-0.046696194],[0.005350804,-0.0775791,-0.025972571,0.03437964,0.06596131,0.011763364,-0.028927868,0.015539136,-0.020678144,-0.04081274,0.010071306,-0.009503053],[0.0143968575,-0.005882642,-0.0021330407,-0.045726813,-0.023908138,-0.101893865,0.004907498,-0.06352033,-0.017150074,-0.057607193,0.009231612,0.07102032],[-0.03979379,-0.0032991832,-0.035502207,-0.09240918,0.06689369,0.008254455,-0.0660145,0.06143858,-0.0029597674,-0.01983029,0.017537657,-0.056026027],[0.0046200724,-0.0011815227,0.015165433,0.004807108,0.024956629,-0.0037617395,-0.03295041,0.027505862,-0.013224971,-0.003859404,-0.02859469,0.012857676],[-0.15720657,0.02887647,0.01013559,0.046655644,-0.03048292,0.021134183,-0.00941202,-0.20611301,-0.012795193,-0.008819375,-0.010136385,-0.07637041],[0.03227111,-0.015019134,-0.0030179762,0.08722455,0.012375917,0.006977763,-0.011820249,0.0034921379,-0.2338191,-0.043302927,0.0001647966,0.0024302443],[0.05298767,-0.019541968,0.030949973,-0.07587251,-0.13171789,-0.2919816,-0.01710671,0.019400202,-0.16470322,0.09574145,0.16563107,0.03686294],[-0.0016020536,-0.00990412,-0.21128221,-0.10198699,-0.0068047824,0.13394651,-0.044938378,-0.10994905,0.017912082,-0.006533374,-0.17883693,0.011871489],[0.009292701,0.013764684,0.0051002167,-0.020638723,0.050920166,-0.4357931,-0.013377832,0.054688245,0.004536082,-0.093294054,-0.02776651,0.1308221],[-0.054239497,-0.0356908,0.008139012,0.07202861,-0.011249155,-0.0073412117,-0.0037481212,-0.04429832,0.038974155,-0.066073224,0.12851556,-0.1275744]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003eNorm. Logit Diff: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Norm. Logit Diff\"}},\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Attribution Patching Over Attention Heads\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1822e1c9-ae24-4173-b390-345980bf73e9');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = px.imshow(\n",
        "    patching_results,\n",
        "    color_continuous_scale=\"RdBu\",\n",
        "    color_continuous_midpoint=0.0,\n",
        "    title=\"Attribution Patching Over Attention Heads\",\n",
        "    labels={\"x\": \"Head\", \"y\": \"Layer\",\"color\":\"Norm. Logit Diff\"},\n",
        "\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1w3bIXx_X2w"
      },
      "source": [
        "Here, we see that the early layer attention heads may not be important for IOI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI0YeTeBsv1E"
      },
      "source": [
        "## 3️⃣ Attribution Patching Over Position\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF3nxNpZqaeL"
      },
      "source": [
        "One benefit of attribution patching is efficiency. Activation patching requires a separate forward pass per activation patched while every attribution patch can be done simultaneously in two forward passes and one backward pass. Attribution patching makes patching much more scalable to large models and can serve as a useful heuristic to find the interesting activations to patch.\n",
        "\n",
        "In practice, whie this approximation is decent when patching in “small” activations like head outputs, performance decreases significantly when patching in “big” activations like those found in the residual stream.\n",
        "\n",
        "Using the same outputs we cached above, we can get the individual contributions at each token position simply by summing across token positions. Although this is messy, it's a quick approximation of the attention mechanism's contribution across token position.\n",
        "\n",
        "*Note: in our specific case here, patching across positions does NOT reflect the entire residual stream, just the post-attention output (i.e., excludes MLPs).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "NswiDYEisyw-"
      },
      "outputs": [],
      "source": [
        "patching_results = []\n",
        "\n",
        "for corrupted_grad, corrupted, clean, layer in zip(\n",
        "    corrupted_grads, corrupted_out, clean_out, range(len(clean_out))\n",
        "):\n",
        "\n",
        "    residual_attr = einops.reduce(\n",
        "        corrupted_grad.value * (clean.value - corrupted.value),\n",
        "        \"batch pos dim -> pos\",\n",
        "        \"sum\",\n",
        "    )\n",
        "\n",
        "    patching_results.append(\n",
        "        residual_attr.detach().cpu().numpy()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JkCnbwK2-VnR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "7yVXil2cs0nv",
        "outputId": "41e7ed4f-7fa3-40c5-be9c-bc3196ff2a14"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"38c6eaf4-ddf6-4264-a1f2-dc649192f124\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"38c6eaf4-ddf6-4264-a1f2-dc649192f124\")) {                    Plotly.newPlot(                        \"38c6eaf4-ddf6-4264-a1f2-dc649192f124\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.0,0.19859776,0.1422194,0.037779175,0.027816411,-0.025727853,-0.06661457,1.331944,0.020576276,-0.0880377,0.50326097,-0.15384659,0.010099655,0.10340737,-0.0034320392,0.19201156],[0.0,0.05628822,-0.09840162,-0.10109313,-0.05423579,0.12361747,-0.1087339,-0.03050007,0.25415656,-0.028010655,-0.06310783,-0.26332977,-0.011289334,-0.13368377,0.16107394,0.20043445],[0.0,0.10953902,-0.20305675,-0.089113966,-0.12107697,-0.07901719,0.033416286,0.48891318,-0.024688914,-0.029062543,-0.0120860115,-0.08306497,0.12700447,-0.3438736,-0.16678391,-0.14033493],[0.0,-0.05625943,-0.08845699,0.12304218,0.003690116,-0.1345139,-0.039301112,-0.09673638,-0.019182026,-0.0979392,-0.05015705,-0.011510663,-0.13801846,-0.22969058,-0.011318062,-0.08426834],[0.0,0.018002508,0.049999833,-0.09046106,0.12488683,-0.07552709,0.05198625,-0.36287695,-0.26720485,-0.34285292,0.08151577,0.08932736,0.15012924,0.056699842,-0.0018051192,-0.121899225],[0.0,-0.019123398,-0.0059723146,0.08494127,0.07717759,-0.021289414,-0.04985226,-0.149731,0.13870311,-0.09117958,0.09868261,0.10973613,-0.2474866,-0.14845793,-0.09729686,-0.21401276],[0.0,-0.008408199,0.00021203887,0.07341764,-0.0034222119,0.009043018,0.050666682,-0.11777635,-0.20832476,-0.059594307,-0.0666658,-0.18739763,-0.2787643,-0.12485623,-0.26428914,-0.088726416],[0.0,0.0089477105,0.018695833,0.03316479,0.024035208,0.051476106,-0.055999007,0.14983703,0.008793814,-0.1329794,-0.07763337,-0.011398135,0.06174268,-0.1511434,-0.20002513,-0.5190315],[0.0,0.009336788,0.01137306,0.0058742873,0.0037288337,0.08420998,0.019560548,-0.09409576,0.015330161,0.08624319,0.12210336,-0.014444653,0.16141549,-0.27604592,-0.14913505,-1.08441],[0.0,0.0028273715,0.006942494,0.006053025,-0.013256458,0.049024567,0.019061051,-0.04550618,-0.2050794,0.3837804,0.060517855,0.014168691,-0.0019706143,-0.05570203,0.24468741,3.212359],[0.0,0.002299946,0.0009810914,-0.0040099495,-0.00014168897,0.017704234,0.014908885,0.010922431,-0.012131959,0.025713444,0.0106506385,-0.003047001,0.011451456,0.11086329,-0.116623454,-0.24215049],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.5141617]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Token Position: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003eNorm. Logit Diff: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Token Position\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Norm. Logit Diff\"}},\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Attribution Patching Over Token Position\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('38c6eaf4-ddf6-4264-a1f2-dc649192f124');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "fig = px.imshow(\n",
        "    patching_results,\n",
        "    color_continuous_scale=\"RdBu\",\n",
        "    color_continuous_midpoint=0.0,\n",
        "    title=\"Attribution Patching Over Token Position\",\n",
        "    labels={\"x\": \"Token Position\", \"y\": \"Layer\",\"color\":\"Norm. Logit Diff\"},\n",
        "\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghs_wKDC_0ts"
      },
      "source": [
        "This result looks similar to our previous result using activation patching but is much less precise, as expected!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJvTRijjrFkK"
      },
      "source": [
        "# Remote Attribution Patching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMBzy0_crFkK"
      },
      "source": [
        "Now that we know how to run an attribution patching experiment in `nnsight`, let's go over how you can use NDIF's publicly-hosted models to further scale your research!\n",
        "\n",
        "We're going to run the same experiment, but now using Llama 3.1 8B. Completing this section of the tutorial will require you to [configure NNsight for remote execution](https://nnsight.net/notebooks/features/remote_execution/) if you haven't already."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5f1EWL5rFkK"
      },
      "source": [
        "## Remote Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "QcS4xYh6rFkK"
      },
      "outputs": [],
      "source": [
        "from nnsight import CONFIG\n",
        "\n",
        "if is_colab:\n",
        "    # include your HuggingFace Token and NNsight API key on Colab secrets\n",
        "    from google.colab import userdata\n",
        "    NDIF_API = userdata.get('NDIF_API')\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "    CONFIG.set_default_api_key(NDIF_API)\n",
        "    !huggingface-cli login -token HF_TOKEN\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN"
      ],
      "metadata": {
        "id": "hG5CsqunyoGn"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "l_yjgNT9rFkK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import nnsight\n",
        "from nnsight import LanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "NDIF_API = userdata.get('NDIF_API')"
      ],
      "metadata": {
        "id": "PB3oczajyfcp"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hyrPRpxWyjYu"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuFcPOCarFkK"
      },
      "source": [
        "Next, let's load the Llama 3.1 8B model, once again using NNsight's `LanguageModel` wrapper. Because we'll be running the model on NDIF's remote servers, no need to specify a `device_map`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "wySGgRW5rFkO",
        "outputId": "8172b8be-d1ad-4cbd-c6af-61ff325da498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
            "  (generator): Generator(\n",
            "    (streamer): Streamer()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "llm = LanguageModel(\"meta-llama/Meta-Llama-3.1-8B\")\n",
        "#llm = LanguageModel(\"mistralai/Mistral-Small-24B-Instruct-2501\")\n",
        "print(llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1hEaIqUrFkP"
      },
      "source": [
        "## IOI Task Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lh82tThrFkP"
      },
      "source": [
        "We've already defined some prompts in the above tutorial, but we'll have to re-tokenize them for Llama 8B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "LeyreEoGrFkP",
        "outputId": "01fd0acc-7141-4bcc-a509-895985470ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mas_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 15 at dim 1 (got 16)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-88979960ac7c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tokenize clean and corrupted inputs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclean_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# The associated corrupted input is the prompt after the current clean prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# for even indices, or the prompt prior to the current clean prompt for odd indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m corrupted_tokens = clean_tokens[\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2866\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2867\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2868\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2870\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m                 )\n\u001b[1;32m   2955\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2956\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2957\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2958\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3156\u001b[0m         )\n\u001b[1;32m   3157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3158\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3159\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msanitized_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eventual_warn_about_too_long_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitized_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitized_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     def _encode_plus(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    791\u001b[0m                         \u001b[0;34m\"Please see if a fast version of this tokenizer is available to have this feature available.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m                     ) from e\n\u001b[0;32m--> 793\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    794\u001b[0m                     \u001b[0;34m\"Unable to create tensor, you should probably activate truncation and/or padding with\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m                     \u001b[0;34m\" 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
          ]
        }
      ],
      "source": [
        "# Tokenize clean and corrupted inputs:\n",
        "clean_tokens = llm.tokenizer(prompts, return_tensors=\"pt\")[\"input_ids\"]\n",
        "# The associated corrupted input is the prompt after the current clean prompt\n",
        "# for even indices, or the prompt prior to the current clean prompt for odd indices\n",
        "corrupted_tokens = clean_tokens[\n",
        "    [(i + 1 if i % 2 == 0 else i - 1) for i in range(len(clean_tokens))]\n",
        "]\n",
        "\n",
        "# Tokenize answers for each prompt:\n",
        "answer_token_indices = torch.tensor(\n",
        "    [\n",
        "        [llm.tokenizer(answers[i][j])[\"input_ids\"][1] for j in range(2)]\n",
        "        for i in range(len(answers))\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nnsight import CONFIG\n",
        "\n",
        "CONFIG.set_default_api_key(\"546253536da74e51a1ccdfe887eafcea\")"
      ],
      "metadata": {
        "id": "Pmd3dKRG0A88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJbICXm7rFkP"
      },
      "source": [
        "Next, we'll establish clean & corrupted baselines for our IOI metric, using the model's clean and corrupted logits and the `get_logit_diff` function defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5YcDNB3rFkP"
      },
      "outputs": [],
      "source": [
        "clean_logits = llm.trace(clean_tokens, trace=False, remote=True)\n",
        "corrupted_logits = llm.trace(corrupted_tokens, trace=False, remote=True)\n",
        "\n",
        "clean_logits = clean_logits['logits']\n",
        "corrupted_logits = corrupted_logits['logits']\n",
        "\n",
        "CLEAN_BASELINE = get_logit_diff(clean_logits, answer_token_indices).item()\n",
        "print(f\"\\n\\nClean logit diff: {CLEAN_BASELINE:.4f}\")\n",
        "\n",
        "CORRUPTED_BASELINE = get_logit_diff(corrupted_logits, answer_token_indices).item()\n",
        "print(f\"Corrupted logit diff: {CORRUPTED_BASELINE:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ywoA0xBrFkP"
      },
      "source": [
        "We've also already defined our `ioi_metric` function. Let's plug in our logit values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8C3a4bVrFkP"
      },
      "outputs": [],
      "source": [
        "print(f\"Clean Baseline is 1: {ioi_metric(clean_logits).item():.4f}\")\n",
        "print(f\"Corrupted Baseline is 0: {ioi_metric(corrupted_logits).item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB66f4y2rFkP"
      },
      "source": [
        "## Remote Attribution Patching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGX6zckgrFkP"
      },
      "source": [
        "Great! We have some baselines. Now, let's run the attribution patching pipeline on Llama 8B. We can't copy the code exactly, because Llama 8B has a different model structure than GPT-2, but we're following the same steps: a clean run and a corrupted run as invokes during one tracing context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe9wJUErrFkQ"
      },
      "outputs": [],
      "source": [
        "clean_out = []\n",
        "corrupted_out = []\n",
        "corrupted_grads = []\n",
        "\n",
        "with llm.trace(remote = True) as tracer:\n",
        "# Using nnsight's tracer.invoke context, we can batch the clean and the\n",
        "# corrupted runs into the same tracing context, allowing us to access\n",
        "# information generated within each of these runs within one forward pass\n",
        "\n",
        "    with tracer.invoke(clean_tokens) as invoker_clean:\n",
        "      # need to set requires grad to true for remote\n",
        "        llm.model.layers[0].self_attn.o_proj.input.requires_grad = True\n",
        "        # Gather each layer's attention\n",
        "        for layer in llm.model.layers:\n",
        "            # Get clean attention output for this layer\n",
        "            # across all attention heads\n",
        "            attn_out = layer.self_attn.o_proj.input\n",
        "            clean_out.append(attn_out.save())\n",
        "\n",
        "    with tracer.invoke(corrupted_tokens) as invoker_corrupted:\n",
        "        # Gather each layer's attention and gradients\n",
        "        for layer in llm.model.layers:\n",
        "            # Get corrupted attention output for this layer\n",
        "            # across all attention heads\n",
        "            attn_out = layer.self_attn.o_proj.input\n",
        "            corrupted_out.append(attn_out.save())\n",
        "            # save corrupted gradients for attribution patching\n",
        "            corrupted_grads.append(attn_out.grad.save())\n",
        "\n",
        "        # Let's get the logits for the model's output\n",
        "        # for the corrupted run\n",
        "        logits = llm.lm_head.output.save()\n",
        "\n",
        "        # Our IOI metric uses tensors saved on cpu, so we\n",
        "        # need to move the logits to cpu.\n",
        "        value = ioi_metric(logits.cpu())\n",
        "\n",
        "        # We also need to run a backwards pass to\n",
        "        # update gradient values\n",
        "        value.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az9D0OMFrFkQ"
      },
      "source": [
        "Awesome! Let's take a look at attention head contributions across layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFv02BgVrFkQ"
      },
      "outputs": [],
      "source": [
        "# format data for plotting across attention heads\n",
        "patching_results = []\n",
        "\n",
        "for corrupted_grad, corrupted, clean, layer in zip(\n",
        "    corrupted_grads, corrupted_out, clean_out, range(len(clean_out))\n",
        "):\n",
        "\n",
        "    residual_attr = einops.reduce(\n",
        "        corrupted_grad.value[:,-1,:] * (clean.value[:,-1,:] - corrupted.value[:,-1,:]),\n",
        "        \"batch (head dim) -> head\",\n",
        "        \"sum\",\n",
        "        head = 32,\n",
        "        dim = 128,\n",
        "    )\n",
        "\n",
        "    patching_results.append(\n",
        "        (residual_attr.float()).detach().numpy()\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNepndSprFkQ"
      },
      "outputs": [],
      "source": [
        "fig = px.imshow(\n",
        "    patching_results,\n",
        "    color_continuous_scale=\"RdBu\",\n",
        "    color_continuous_midpoint=0.0,\n",
        "    title=\"Attribution Patching Over Attention Heads\",\n",
        "    labels={\"x\": \"Head\", \"y\": \"Layer\",\"color\":\"Norm. Logit Diff\"},\n",
        "\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSTNUbq0rFkQ"
      },
      "source": [
        "Next, let's check out the contribution of the residual stream over token position across layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX27tAS8rFkQ"
      },
      "outputs": [],
      "source": [
        "# format data for plotting across input tokens\n",
        "patching_results = []\n",
        "\n",
        "for corrupted_grad, corrupted, clean, layer in zip(\n",
        "    corrupted_grads, corrupted_out, clean_out, range(len(clean_out))\n",
        "):\n",
        "\n",
        "    residual_attr = einops.reduce(\n",
        "        corrupted_grad.value * (clean.value - corrupted.value),\n",
        "        \"batch pos dim -> pos\",\n",
        "        \"sum\",\n",
        "    )\n",
        "\n",
        "    patching_results.append(\n",
        "        (residual_attr.detach().cpu().float()).numpy()\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKVvnvhPrFkQ"
      },
      "outputs": [],
      "source": [
        "fig = px.imshow(\n",
        "    patching_results,\n",
        "    color_continuous_scale=\"RdBu\",\n",
        "    color_continuous_midpoint=0.0,\n",
        "    title=\"Attribution Patching Over Token Position\",\n",
        "    labels={\"x\": \"Token Position\", \"y\": \"Layer\",\"color\":\"Norm. Logit Diff\"},\n",
        "\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tAVXulfrFkQ"
      },
      "source": [
        "Great! We've now successfully performed an attribution patching experiment on GPT-2 and Llama 8b."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}